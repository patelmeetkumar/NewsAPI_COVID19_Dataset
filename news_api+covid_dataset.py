# -*- coding: utf-8 -*-
"""News_API+Covid_Dataset.ipynb

Automatically generated by Colaboratory.

Name: Meetkumar Patel  
HW5 News API + COVID-19 Dataset  
CPP CS 4650.01
"""

!pip install spacy

!python -m spacy download en_core_web_lg

"""## Restart runtime NOW"""

import spacy
nlp_eng = spacy.load('en_core_web_lg')

!pip install newsapi-python
from newsapi import NewsApiClient

# All the rest import statements
import pickle
import pandas as pd
import string
from collections import Counter
from wordcloud import WordCloud 
import matplotlib.pyplot as plt

newsapi = NewsApiClient (api_key='API_KEY_HERE')

# Get news articles details with a specified query.
def get_news(pagina):
  temp = newsapi.get_everything(q='coronavirus', language='en',
                              from_param='2020-09-27', to='2020-10-27', 
                              sort_by='relevancy', page=pagina)
  return temp

articles = list(map(get_news, range(1,6)))

# Use pickle and save a file
filename = 'articlesCOVID.pckl'
pickle.dump(articles, open(filename, 'wb'))

filename = 'articlesCOVID.pckl'
loaded_model = pickle.load(open(filename, 'rb'))

filepath = '/content/articlesCOVID.pckl'
pickle.dump(loaded_model, open(filepath, 'wb'))

# Iterate through every article to retrieve required information and save in df.
dados, titles, dates, descriptions = ([] for i in range(4)) 
counter = 0
for i, article in enumerate(articles):
    for x in article['articles']:
        title = x['title']
        titles.append(title)
        description = x['description']
        descriptions.append(description)
        content = x['content']
        date = x['publishedAt']
        dates.append(date)
        dados.append({'title':titles[counter], 'date':dates[counter], 'desc':descriptions[counter], 'content':content})
        counter += 1
df = pd.DataFrame(dados)
df = df.dropna()
df.shape

# Retrieve useful deemed keywords from the article's content.
def get_keywords_eng(content):
    result = []
    pos_tag = ['NOUN','VERB','PROPN']
    for token in nlp_eng(content):
        if (token.text in nlp_eng.Defaults.stop_words or token.text in string.punctuation):
            continue
        if (token.pos_ in pos_tag):
            result.append(token.text)
    return result

# Add the five keywords per article to df.
results = []
for content in df.content.values:
    results.append([('#' + x[0]) for x in Counter(get_keywords_eng(content)).most_common(5)])
df['keywords'] = results

# Save the dataset
df.to_csv('dataset.csv')
df

# Print five most common words for every article
text = str(results)
print(text)

# Create a wordcloud of the keywords
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()